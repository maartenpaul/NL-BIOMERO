[SSH]
# -------------------------------------
# SSH settings
# -------------------------------------
# The alias for the SLURM SSH connection
host=localslurm
# Set the rest of your SSH configuration in your SSH config under this host name/alias
# Or in e.g. /etc/fabric.yml (see Fabric's documentation for details on config loading)

[SLURM]
# -------------------------------------
# Slurm settings
# -------------------------------------
# General settings for where to find things on the Slurm cluster.
# -------------------------------------
# PATHS
# -------------------------------------
# The path on SLURM entrypoint for storing datafiles
#
# Note: 
# This example is NOT relative to the Slurm user's home dir
slurm_data_path=/data/my-scratch/data
# The path on SLURM entrypoint for storing container image files
#
# Note: 
# This example is NOT relative to the Slurm user's home dir
slurm_images_path=/data/my-scratch/singularity_images/workflows
# The path on SLURM entrypoint for storing the slurm job scripts
#
# Note: 
# This example is NOT relative to the Slurm user's home dir
slurm_script_path=/data/my-scratch/slurm-scripts
# -------------------------------------
# REPOSITORIES
# -------------------------------------
# A (github) repository to pull the slurm scripts from.
#
# Note: 
# If you provide no repository, we will generate scripts instead!
# Based on the job_template and the descriptor.json
#
# Example:
# slurm_script_repo=https://github.com/Cellular-Imaging-Amsterdam-UMC/slurm-scripts/
slurm_script_repo=

# -------------------------------------
# Processing settings
# -------------------------------------
# General/default settings for processing jobs.
# Note: NOT YET IMPLEMENTED
# Note: If you need to change it for a specific case only,
# you should change the job script instead, either in Omero or Slurm 

[ANALYTICS]
# =====================================
# Analytics Settings
# =====================================
# General settings to control workflow tracking 
# and listeners for detailed monitoring and insights.

# -------------------------------------
# WORKFLOW TRACKER SETTINGS
# -------------------------------------
# The workflow tracker collects and logs information 
# on workflow execution, job statuses, and related analytics.
# This is the main switch to enable or disable workflow 
# tracking as a whole.
#
# If disabled, none of the listeners below will be 
# activated, regardless of their individual settings.
#
# Options: True, False
track_workflows=True

# -------------------------------------
# DATABASE CONFIGURATION
# -------------------------------------
# SQLAlchemy database connection URL for persisting 
# workflow analytics data. This setting allows configuring 
# the database connection for storing the tracking and analytics 
# data. If no value is set here, environment variables 
# will be used as the default.
# 
# Example:
# sqlalchemy_url=postgresql+psycopg2://user:password@localhost:5432/yourdatabase
#
# See https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls
# for more info and examples of database URLs supported by sqlalchemy
#
# Note: If SQLALCHEMY_URL is set as an environment variable, 
# it will override this setting.
# sqlalchemy_url=

# -------------------------------------
# LISTENER SETTINGS
# -------------------------------------
# Listeners provide detailed monitoring and insights for 
# specific aspects of workflow execution. Each listener 
# can be enabled or disabled independently.
#
# NOTE: The listeners below will only be active if 
# track_workflows=True.

# -------------------------------------
# JOB ACCOUNTING LISTENER
# -------------------------------------
# Monitors job accounting data such as resource usage 
# (CPU, memory) and SLURM job states (completed, failed).
#
# Options: True, False
enable_job_accounting=False

# -------------------------------------
# JOB PROGRESS LISTENER
# -------------------------------------
# Tracks the progress of SLURM jobs, capturing intermediate 
# statuses for real-time insights into job execution.
#
# Options: True, False
enable_job_progress=False

# -------------------------------------
# WORKFLOW ANALYTICS LISTENER
# -------------------------------------
# Provides detailed insights into workflow performance, 
# including execution times, bottlenecks, and overall 
# efficiency.
#
# Options: True, False
enable_workflow_analytics=False


[CONVERTERS]
# -------------------------------------
# Converters settings
# -------------------------------------
# Settings for linking to external converters.
# 
# By default, BIOMERO exports images as ZARR to the HPC.
# But, the workflow you want to execute might require 
# a different filetype. E.g. most of our example workflows
# require TIFF input files. 
#
# By default we will build a converter on Slurm for you.
# Theoretically you can add other converters here to pull
# those instead. These should be available on dockerhub.
# 
# -------------------------------------
# ZARR TO TIFF
# -------------------------------------
# Uncomment this if you want to pull the image instead of 
# build it. E.g. if you don't have singularity build rights
# on your Slurm.
#
# Please pin it to a specific version to reduce unforeseen errors.
#
# Key should be the types "X_to_Y" and value should be the docker image
# zarr_to_tiff=cellularimagingcf/convert_zarr_to_tiff:1.14.0

[MODELS]
# -------------------------------------
# Model settings
# -------------------------------------
# Settings for models/singularity images that we want to run on Slurm
#
# NOTE: keys have to be unique, and require a <key>_repo and <key>_image value as well.
#
# NOTE 2: Versions for the repo are highly encouraged! 
# Latest/master can change and cause issues with reproducability!
# We pickup the container version based on the version of the repository.
# For generic master branch, we pick up generic latest container.
# -------------------------------------
# CELLPOSE SEGMENTATION
# -------------------------------------
# The path to store the container on the slurm_images_path
cellpose=cellpose
# The (e.g. github) repository with the descriptor.json file
cellpose_repo=https://github.com/TorecLuik/W_NucleiSegmentation-Cellpose/tree/v1.3.1
# The jobscript in the 'slurm_script_repo'
cellpose_job=jobs/cellpose.sh
# Override the default job values for this workflow
# Or add a job value to this workflow
# For more examples of such parameters, google SBATCH parameters.
# If you don't want to override, comment out / delete the line.
# Run CellPose Slurm with medium GPU
# cellpose_job_gres=gpu:1g.10gb:1
# Run CellPose Slurm with more GB CPU memory
# cellpose_job_mem=15GB
# -------------------------------------
# STARDIST SEGMENTATION
# -------------------------------------
# The path to store the container on the slurm_images_path
stardist=stardist
# The (e.g. github) repository with the descriptor.json file
stardist_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-Stardist/tree/v1.3.2
# The jobscript in the 'slurm_script_repo'
stardist_job=jobs/stardist.sh
# -------------------------------------
# CELLPROFILER SEGMENTATION
# # -------------------------------------
# # The path to store the container on the slurm_images_path
# cellprofiler=cellprofiler
# # The (e.g. github) repository with the descriptor.json file
# cellprofiler_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler/tree/v1.6.4
# # The jobscript in the 'slurm_script_repo'
# cellprofiler_job=jobs/cellprofiler.sh
# -------------------------------------
# DEEPCELL SEGMENTATION
# # -------------------------------------
# # The path to store the container on the slurm_images_path
# deepcell=deepcell
# # The (e.g. github) repository with the descriptor.json file
# deepcell_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-DeepCell/tree/v.1.4.3
# # The jobscript in the 'slurm_script_repo'
# deepcell_job=jobs/deepcell.sh
# -------------------------------------
# IMAGEJ SEGMENTATION
# # -------------------------------------
# # The path to store the container on the slurm_images_path
# imagej=imagej
# # The (e.g. github) repository with the descriptor.json file
# imagej_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-ImageJ/tree/v1.12.10
# # The jobscript in the 'slurm_script_repo'
# imagej_job=jobs/imagej.sh
# # -------------------------------------
# # CELLPROFILER SPOT COUNTING
# # -------------------------------------
# # The path to store the container on the slurm_images_path
# cellprofiler_spot=cellprofiler_spot
# # The (e.g. github) repository with the descriptor.json file
# cellprofiler_spot_repo=https://github.com/TorecLuik/W_SpotCounting-CellProfiler/tree/v1.0.1
# # The jobscript in the 'slurm_script_repo'
# cellprofiler_spot_job=jobs/cellprofiler_spot.sh
# # -------------------------------------
# CELLEXPANSION
# -------------------------------------
# The path to store the container on the slurm_images_path
cellexpansion=cellexpansion
# The (e.g. github) repository with the descriptor.json file
cellexpansion_repo=https://github.com/TorecLuik/W_CellExpansion/tree/v2.0.1
# The jobscript in the 'slurm_script_repo'
cellexpansion_job=jobs/cellexpansion.sh
# # -------------------------------------
# SPOT COUNTING
# -------------------------------------
# The path to store the container on the slurm_images_path
spotcounting=spotcounting
# The (e.g. github) repository with the descriptor.json file
spotcounting_repo=https://github.com/TorecLuik/W_CountMaskOverlap/tree/v1.0.1
# The jobscript in the 'slurm_script_repo'
spotcounting_job=jobs/spotcounting.sh
# -------------------------------------
# MEASUREMENTS - 2 masks (cell + nucleus)
# -------------------------------------
# The path to store the container on the slurm_images_path
nuclei_measurements=nuclei_measurements
# The (e.g. github) repository with the descriptor.json file
nuclei_measurements_repo=https://github.com/Cellular-Imaging-Amsterdam-UMC/W_Measurements-Nuclei-CellProfiler/tree/v1.0.0
# The jobscript in the 'slurm_script_repo'
nuclei_measurements_job=jobs/nuclei_measurements.sh
# Run Slurm with more GB CPU memory
# nuclei_measurements_job_mem=32GB
# Run Slurm with higher timeout value (d-hh:mm:ss)
# nuclei_measurements_job_time=01:00:00
# -------------------------------------
# MEASUREMENTS - 3 masks (cell + nucleus + aggregate)
# -------------------------------------
# The path to store the container on the slurm_images_path
aggregates_measurements=aggregates_measurements
# The (e.g. github) repository with the descriptor.json file
aggregates_measurements_repo=https://github.com/Cellular-Imaging-Amsterdam-UMC/W_Measurements-CellProfiler/tree/v1.1.0
# The jobscript in the 'slurm_script_repo'
aggregates_measurements_job=jobs/aggregates_measurements.sh
# Run Slurm with more GB CPU memory
# aggregates_measurements_job_mem=32GB
# Run Slurm with higher timeout value (d-hh:mm:ss)
# aggregates_measurements_job_time=01:00:00
 
